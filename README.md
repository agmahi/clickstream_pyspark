# clickstream_pyspark
pyspark streaming app to process clickstream data and publish it to a dashboard

# Set Up
Install docker to run kafka locally. 
* Once docker is installed, create a compose.yaml to manage your Docker components (services, volumes, networks, etc.)
* Run `docker compose up -d` from the directory that has your yaml file

**References:** [Confluent Guide](https://developer.confluent.io/confluent-tutorials/kafka-on-docker/?utm_medium=sem&utm_source=google&utm_campaign=ch.sem_br.nonbrand_tp.prs_tgt.dsa_mt.dsa_rgn.namer_lng.eng_dv.all_con.confluent-developer&utm_term=&creative=&device=c&placement=&gad_source=1&gad_campaignid=19560855036&gbraid=0AAAAADRv2c3vRPcAhbiLx4cxC1ijGFFHl&gclid=Cj0KCQjw0qTCBhCmARIsAAj8C4aJenxSKW9Wx0eg20LtQ6d98IXbnHs9HFi5_XQ2A6JqM4rR5zjH9zsaAkjvEALw_wcB)

